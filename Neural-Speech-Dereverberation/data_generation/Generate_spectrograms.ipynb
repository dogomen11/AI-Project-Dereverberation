{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POaUAfdhP4vF",
        "outputId": "f3e81d86-6d0f-4665-d442-7e1de4682bfe"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display as display\n",
        "import librosa.feature\n",
        "import soundfile as sf\n",
        "from scipy.signal import resample\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import cv2\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################################\n",
        "# audio generation utils\n",
        "##################################\n",
        "\n",
        "def extract_audio(filename):\n",
        "    \"\"\"\n",
        "    Extract audio given the filename (.wav, .flac, etc format)\n",
        "    \"\"\"\n",
        "\n",
        "    audio, rate = sf.read(filename, always_2d=True)\n",
        "    audio = np.reshape(audio, (1, -1))\n",
        "    audio = audio[0]\n",
        "    time = np.linspace(0, len(audio)/rate, len(audio), endpoint=False)\n",
        "    return audio, time, rate\n",
        "\n",
        "def generate_spec(audio_sequence, rate, n_fft=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    Generate spectrogram using librosa\n",
        "    audio_sequence: list representing waveform\n",
        "    rate: sampling rate (16000 for all LibriSpeech audios)\n",
        "    nfft and hop_length: stft parameters\n",
        "    \"\"\"\n",
        "    S = librosa.feature.melspectrogram(audio_sequence, sr=rate, n_fft=n_fft, hop_length=hop_length, n_mels=128, fmin=20,\n",
        "                                       fmax=8300)\n",
        "    log_spectra = librosa.power_to_db(S, ref=np.mean, top_db=80)\n",
        "    return log_spectra\n",
        "\n",
        "def reconstruct_wave(spec, rate=16000, normalize_data=False):\n",
        "    \"\"\"\n",
        "    Reconstruct waveform\n",
        "    spec: spectrogram generated using Librosa\n",
        "    rate: sampling rate\n",
        "    \"\"\"\n",
        "    power = librosa.db_to_power(spec, ref=5.0)\n",
        "    audio = librosa.feature.inverse.mel_to_audio(power, sr=rate, n_fft=2048, hop_length=512)\n",
        "    out_audio = audio / np.max(audio) if normalize_data else audio\n",
        "    return out_audio\n",
        "\n",
        "def normalize(spec, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Normalize spectrogram with zero mean and unitary variance\n",
        "    spec: spectrogram generated using Librosa\n",
        "    \"\"\"\n",
        "\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    return spec_norm, (mean, std)\n",
        "\n",
        "def minmax_scaler(spec):\n",
        "    \"\"\"\n",
        "    min max scaler over spectrogram\n",
        "    \"\"\"\n",
        "    spec_max = np.max(spec)\n",
        "    spec_min = np.min(spec)\n",
        "\n",
        "    return (spec-spec_min)/(spec_max - spec_min), (spec_max, spec_min)\n",
        "\n",
        "def linear_scaler(spec):\n",
        "    \"\"\"\n",
        "    linear scaler over spectrogram\n",
        "    min value -> -1 and max value -> 1\n",
        "    \"\"\"\n",
        "    spec_max = np.max(spec)\n",
        "    spec_min = np.min(spec)\n",
        "    m = 2/(spec_max-spec_min)\n",
        "    n = (spec_max + spec_min)/(spec_min-spec_max)\n",
        "\n",
        "    return m*spec + n, (m, n)\n",
        "\n",
        "def split_specgram(example, clean_example, frames = 11):\n",
        "    \"\"\"\n",
        "    Split specgram in groups of frames, the purpose is prepare data for the LSTM model input\n",
        "\n",
        "    example: reverberant spectrogram\n",
        "    clean_example: clean or target spectrogram\n",
        "\n",
        "    return data input to the LSTM model and targets\n",
        "    \"\"\"\n",
        "    clean_spec = clean_example[0, :, :]\n",
        "    rev_spec = example[0, :, :]\n",
        "\n",
        "    n, m = clean_spec.shape\n",
        "\n",
        "    targets = torch.zeros((m-frames+1, n))\n",
        "    data = torch.zeros((m-frames+1, n*frames))\n",
        "  \n",
        "    idx_target = frames//2\n",
        "    for i in range(m-frames+1):\n",
        "        try:\n",
        "            targets[i, :] = clean_spec[:, idx_target]\n",
        "            data[i, :] = torch.reshape(rev_spec[:, i:i+frames], (1, -1))[0, :]\n",
        "            idx_target += 1\n",
        "        except (IndexError):\n",
        "            pass\n",
        "    return data, targets\n",
        "\n",
        "def split_realdata(example, frames = 11):\n",
        "    \n",
        "    \"\"\"\n",
        "    Split 1 specgram in groups of frames, the purpose is prepare data for the LSTM and MLP model input\n",
        "\n",
        "    example: reverberant ''real'' (not simulated) spectrogram\n",
        "\n",
        "    return data input to the LSTM or MLP model \n",
        "    \"\"\"\n",
        "  \n",
        "    rev_spec = example[0, :, :]\n",
        "    n, m = rev_spec.shape\n",
        "    data = torch.zeros((m-frames+1, n*frames))\n",
        "    for i in range(m-frames+1):\n",
        "        data[i, :] = torch.reshape(rev_spec[:, i:i+frames], (1, -1))[0, :]\n",
        "    return data\n",
        "\n",
        "def prepare_data(X, y, display = False):\n",
        "\n",
        "    \"\"\"\n",
        "    Use split_specgram to split all specgrams\n",
        "    X: tensor containing reverberant spectrograms\n",
        "    y: tensor containing target spectrograms\n",
        "    \"\"\"\n",
        "\n",
        "    data0, target0 = split_specgram(X[0, :, :, :], y[0, :, :, :])\n",
        "\n",
        "    total_data = data0.cuda()\n",
        "    targets = target0.cuda()\n",
        "  \n",
        "    for i in range(1, X.shape[0]):\n",
        "           if display: \n",
        "               print(\"Specgram n°\" + str(i)) \n",
        "\n",
        "           data_i, target_i = split_specgram(X[i, :, :, :], y[i, :, :, :])\n",
        "           total_data = torch.cat((total_data, data_i.cuda()), 0)\n",
        "           targets = torch.cat((targets, target_i.cuda()), 0)\n",
        "\n",
        "    return  total_data, targets\n",
        "\n",
        "\n",
        "def split_for_supression(rev_tensor, target_tensor):\n",
        "    \"\"\"\n",
        "    Given reverberant and target tensor with shape (#examples, 1, 128, 340)\n",
        "    return tensors with the same information, but with shape (#examples*340, 128)\n",
        "    \"\"\"\n",
        "    rev_transform = torch.tensor([])\n",
        "    target_transform = torch.tensor([])\n",
        "\n",
        "    for example in range(rev_tensor.shape[0]):\n",
        "        rev_transform = torch.cat((rev_transform, rev_tensor[example, 0, :, :].T))\n",
        "    \n",
        "    if (target_tensor!=None):\n",
        "        for example in range(target_tensor.shape[0]):\n",
        "            target_transform = torch.cat((target_transform, target_tensor[example, 0, :, :].T))\n",
        "  \n",
        "    return rev_transform, target_transform\n",
        "\n",
        "def normalize_per_frame(spec_transpose):\n",
        "    \"\"\"\n",
        "    Normalize over spectrogram rows\n",
        "    \"\"\"\n",
        "    means = []\n",
        "    stds = []\n",
        "    norm_spec = torch.zeros(spec_transpose.shape)\n",
        "\n",
        "    for spec_row in range(norm_spec.shape[0]):\n",
        "        current_mean = spec_transpose[spec_row, :].mean()\n",
        "        current_std = spec_transpose[spec_row, :].std()\n",
        "        means.append(current_mean)\n",
        "        stds.append(current_std)\n",
        "        norm_spec[spec_row, :] = (spec_transpose[spec_row, :]- current_mean)/(current_std+1e-6) \n",
        "  \n",
        "    return norm_spec, (means, stds)\n",
        "\n",
        "def denormalize_per_frame(norm_spec_transpose, means, stds):\n",
        "    \"\"\"\n",
        "    denormalize row by row using means and stds given by normalize_per_frame\n",
        "    \"\"\"\n",
        "    denorm_spec = torch.zeros(norm_spec_transpose.shape)\n",
        "\n",
        "    for spec_row in range(norm_spec_transpose.shape[0]):\n",
        "        denorm_spec[spec_row, :] = (norm_spec_transpose[spec_row, :])*(stds[spec_row] + 1e-6) + means[spec_row]\n",
        "    \n",
        "    return denorm_spec.T\n",
        "\n",
        "\n",
        "#################################\n",
        "# reverberation utils\n",
        "#################################\n",
        "\n",
        "def zero_pad(x, k):\n",
        "    \"\"\"\n",
        "    add k zeros to x signal\n",
        "    \"\"\"\n",
        "    return np.append(x, np.zeros(k))\n",
        "\n",
        "\n",
        "def awgn(signal, regsnr):\n",
        "    \"\"\"\n",
        "    add random noise to signal\n",
        "    regsnr: signal to noise ratio\n",
        "    \"\"\"\n",
        "    sigpower = sum([math.pow(abs(signal[i]), 2) for i in range(len(signal))])\n",
        "    sigpower = sigpower / len(signal)\n",
        "    noisepower = sigpower / (math.pow(10, regsnr / 10))\n",
        "    sample = np.random.normal(0, 1, len(signal))\n",
        "    noise = math.sqrt(noisepower) * sample\n",
        "    return noise\n",
        "\n",
        "\n",
        "def discrete_conv(x, h, x_fs, h_fs, snr=30, aug_factor=1):\n",
        "    \"\"\"\n",
        "    Convolution using fft\n",
        "    x: speech waveform\n",
        "    h: RIR waveform\n",
        "    x_fs: speech signal sampling rate (if is not 16000 the signal will be resampled)\n",
        "    h_fs: RIR signal sampling rate (if is not 16000 the signal will be resampled)\n",
        "\n",
        "    Based on https://github.com/vtolani95/convolution/blob/master/reverb.py\n",
        "    \"\"\"\n",
        "\n",
        "    numSamples_h = round(len(h) / h_fs * 16000)\n",
        "    numSamples_x = round(len(x) / x_fs * 16000)\n",
        "\n",
        "    if h_fs != 16000:\n",
        "        h = resample(h, numSamples_h) # resample RIR\n",
        "\n",
        "    if x_fs != 16000:\n",
        "        x = resample(x, numSamples_x) # resample speech signal\n",
        "\n",
        "    L, P = len(x), len(h)\n",
        "    h_zp = zero_pad(h, L - 1)\n",
        "    x_zp = zero_pad(x, P - 1)\n",
        "    X = np.fft.fft(x_zp)\n",
        "    output = np.fft.ifft(X * np.fft.fft(h_zp)).real\n",
        "    output = aug_factor * output + x_zp\n",
        "    output = output + awgn(output, snr)\n",
        "    return output\n",
        "\n",
        "###################################\n",
        "#plot utils\n",
        "###################################\n",
        "\n",
        "def graph_spec(spec, rate=16000, title=False):\n",
        "    \"\"\"\n",
        "    plot spectrogram\n",
        "    spec: spectrogram generated using Librosa\n",
        "    rate: sampling rate\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    display.specshow(spec, sr=rate, y_axis='mel', x_axis='time')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    if (title):\n",
        "        plt.title('Log-Power spectrogram')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_time_wave(audio, rate=16000):\n",
        "    \"\"\"\n",
        "    plot waveform given speech audio\n",
        "    audio: array containing waveform\n",
        "    rate: sampling rate\n",
        "\n",
        "    \"\"\"\n",
        "    time = np.linspace(0, len(audio)/rate, len(audio), endpoint=False)\n",
        "    plt.figure()\n",
        "    plt.plot(time, audio)\n",
        "    plt.xlabel(\"Time (secs)\")\n",
        "    plt.ylabel(\"Power\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJX3le06MuP"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND5GGCbW-19e",
        "outputId": "0f05d6e2-0b9e-43c1-b097-9e893666f810"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grTzKBGQ6Ur4",
        "outputId": "da0a18c5-c327-4379-b51d-2baf48092b81"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
        "!tar -xf train-clean-100.tar.gz\n",
        "!mv LibriSpeech/ LibriSpeechTrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DUMgMV7ah4a",
        "outputId": "81b6748e-6afe-4e85-a561-03a992e46f08"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "!tar -xf test-clean.tar.gz\n",
        "!mv LibriSpeech/ LibriSpeechTest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQxAjDTUAIK7",
        "outputId": "15c7b60c-1289-4859-8090-64998f420d9f"
      },
      "outputs": [],
      "source": [
        "!wget isophonics.net/files/irs/classroomOmni.zip\n",
        "!unzip classroomOmni\n",
        "!mv Omni/ ClassroomOmni/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3fANzrBAl4F",
        "outputId": "0afdb8e1-fcfc-45b1-e325-9ec61acb440a"
      },
      "outputs": [],
      "source": [
        "!wget isophonics.net/files/irs/octagonOmni.zip\n",
        "!unzip octagonOmni\n",
        "!mv Omni/ OctagonOmni/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uni_ygTA8P3",
        "outputId": "a2d76c75-b469-4cf3-a891-04cdf7f35aa7"
      },
      "outputs": [],
      "source": [
        "!wget isophonics.net/files/irs/greathallOmni.zip\n",
        "!unzip greathallOmni\n",
        "!mv Omni/ GreatHallOmni/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3miqc4AIAtNw",
        "outputId": "abbe0fcd-132e-4af9-ee4b-5029275c6450"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LALnosoVQ7Pf"
      },
      "outputs": [],
      "source": [
        "def train_data(audio_dir, rir_dir, lower_bound, upper_bound, checkpointX, checkpointY):\n",
        "    \"\"\"\n",
        "    Read training data generating reverberant waveforms and spectrograms\n",
        "   \n",
        "    audio_dir: directory containing the speech audio files\n",
        "    rir_dir: directory containing RIRs audio files\n",
        "    lower_bound: initial example to be considered\n",
        "    upper_bound: final example to be considered\n",
        "    checkpointX: directory + filename to save reverberant data\n",
        "    checkpointY: directory + filename to save target data \n",
        "    \"\"\"\n",
        "\n",
        "    sys.path.append(audio_dir)\n",
        "    sys.path.append(rir_dir)\n",
        "    \n",
        "    rir_file_names = []\n",
        "    for subdir, dirs, files in os.walk(rir_dir):\n",
        "        for file in files:\n",
        "            if (\".wav\" in file):\n",
        "                rir_file_names.append(os.path.join(subdir,file))\n",
        "\n",
        "    audio_file_names = []\n",
        "    for subdir, dirs, files in os.walk(audio_dir):\n",
        "        for file in files:\n",
        "            if (\".flac\" in file):\n",
        "                audio_file_names.append(os.path.join(subdir,file))\n",
        "    \n",
        "    print (\"RIRs found: \" + str(len(rir_file_names)))\n",
        "    print (\"Audio files found: \" + str(len(audio_file_names)))\n",
        "    time_size = 340\n",
        "    frequency_size = 128\n",
        "    X = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    y = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    for i in range(lower_bound, upper_bound):\n",
        "        rir_index = random.sample(range(len(rir_file_names)), 1)[0]\n",
        "        ir_audio, ir_time, ir_rate = extract_audio(rir_file_names[rir_index])\n",
        "\n",
        "        speech_audio, speech_time, speech_rate = extract_audio(audio_file_names[i])\n",
        "        speech_spec = generate_spec(speech_audio, speech_rate)\n",
        "        \n",
        "        random_snr = random.sample(range(15, 36), 1)[0]\n",
        "        speech_rev = discrete_conv(speech_audio, ir_audio, 16000, 96000, snr = random_snr)\n",
        "        speech_rev = speech_rev[0:len(speech_audio)]\n",
        "        rev_spec = generate_spec(speech_rev, speech_rate)\n",
        "        \n",
        "        speech_spec = cv2.resize(speech_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "        rev_spec = cv2.resize(rev_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "\n",
        "        print(\"Proccesing audio file n°: \" + str(i+1))\n",
        "        X[i-lower_bound, 0, :, :] = torch.tensor(rev_spec)\n",
        "        y[i-lower_bound, 0, :, :] = torch.tensor(speech_spec)\n",
        "\n",
        "        if ((i+1)%500 == 0):\n",
        "          torch.save(X, checkpointX)\n",
        "          torch.save(y, checkpointY)\n",
        "          print('Saved data')\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4dKFVSUAHu",
        "outputId": "97c42e95-a9ca-4c4b-92bf-b18136271317"
      },
      "outputs": [],
      "source": [
        "rir_rootdir1 = 'GreatHallOmni/'\n",
        "audio_rootdir1 = 'LibriSpeechTrain/'\n",
        "checkpointX1 = '/content/drive/My Drive/data_audio/non_norm_data/X_train_1.pth'\n",
        "checkpointY1 = '/content/drive/My Drive/data_audio/non_norm_data/y_train_1.pth'\n",
        "_, _ = train_data(audio_rootdir1, rir_rootdir1, 0, 5000, checkpointX1, checkpointY1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2fVAZl6v3t",
        "outputId": "97bc3ce4-d153-473d-9514-8e35b89414be"
      },
      "outputs": [],
      "source": [
        "rir_rootdir2 = 'OctagonOmni/'\n",
        "audio_rootdir2 = 'LibriSpeechTrain/'\n",
        "checkpointX2 = '/content/drive/My Drive/data_audio/non_norm_data/X_train_2.pth'\n",
        "checkpointY2 = '/content/drive/My Drive/data_audio/non_norm_data/y_train_2.pth'\n",
        "_, _ = train_data(audio_rootdir2, rir_rootdir2, 5000, 10000, checkpointX2, checkpointY2)\n",
        "\n",
        "rir_rootdir3 = 'GreatHallOmni/'\n",
        "audio_rootdir3 = 'LibriSpeechTrain/'\n",
        "checkpointX3 = '/content/drive/My Drive/data_audio/non_norm_data/X_train_3.pth'\n",
        "checkpointY3 = '/content/drive/My Drive/data_audio/non_norm_data/y_train_3.pth'\n",
        "_, _ = train_data(audio_rootdir3, rir_rootdir3, 10000, 15000, checkpointX3, checkpointY3)\n",
        "\n",
        "rir_rootdir4 = 'OctagonOmni/'\n",
        "audio_rootdir4 = 'LibriSpeechTrain/'\n",
        "checkpointX4 = '/content/drive/My Drive/data_audio/non_norm_data/X_train_4.pth'\n",
        "checkpointY4 = '/content/drive/My Drive/data_audio/non_norm_data/y_train_4.pth'\n",
        "_, _ = train_data(audio_rootdir4, rir_rootdir4, 15000, 20000, checkpointX4, checkpointY4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QphzU4ZnE7_2"
      },
      "outputs": [],
      "source": [
        "def test_data(audio_dir, rir_dir, lower_bound, upper_bound, checkpoints, noise = [15, 35]):\n",
        "\n",
        "    \"\"\"\n",
        "    read test data generating reverberant spectrograms and waveforms\n",
        "    \n",
        "    audio_dir: directory containing speech audio files\n",
        "    rir_dir: directory\n",
        "    lower_bound: initial example to be considered\n",
        "    upper_bound: final example to be considered\n",
        "    checkpoints: list containing directories for save rev spectrogram, target spectrogram\n",
        "                rev waveforms and target waveforms respectively\n",
        "    noise: add noise with random snr in [noise[0], noise[1]]\n",
        "    \"\"\"\n",
        "    \n",
        "    checkpointX = checkpoints[0]\n",
        "    checkpointY = checkpoints[1]\n",
        "    checkpoint_waverev = checkpoints[2]\n",
        "    checkpoint_wavetarget = checkpoints[3]\n",
        "    \n",
        "    sys.path.append(audio_dir)\n",
        "    sys.path.append(rir_dir)\n",
        "    \n",
        "    rir_file_names = []\n",
        "    for subdir, dirs, files in os.walk(rir_dir):\n",
        "        for file in files:\n",
        "            if (\".wav\" in file):\n",
        "                rir_file_names.append(os.path.join(subdir,file))\n",
        "\n",
        "    audio_file_names = []\n",
        "    for subdir, dirs, files in os.walk(audio_dir):\n",
        "        for file in files:\n",
        "            if (\".flac\" in file):\n",
        "                audio_file_names.append(os.path.join(subdir,file))\n",
        "    \n",
        "    print (\"RIRs found: \" + str(len(rir_file_names)))\n",
        "    print (\"Audio files found: \" + str(len(audio_file_names)))\n",
        "    time_size = 340\n",
        "    frequency_size = 128\n",
        "    X = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    y = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "\n",
        "    wave_data = []\n",
        "    wave_targets = []\n",
        "\n",
        "\n",
        "    for i in range(lower_bound, upper_bound):\n",
        "        rir_index = random.sample(range(len(rir_file_names)), 1)[0]\n",
        "        ir_audio, ir_time, ir_rate = extract_audio(rir_file_names[rir_index])\n",
        "\n",
        "        speech_audio, speech_time, speech_rate = extract_audio(audio_file_names[i])\n",
        "        wave_targets.append(speech_audio)\n",
        "        speech_spec = generate_spec(speech_audio, speech_rate)\n",
        "        \n",
        "        random_snr = random.sample(range(noise[0], noise[1]), 1)[0]\n",
        "        speech_rev = discrete_conv(speech_audio, ir_audio, 16000, 96000, snr = random_snr)\n",
        "        speech_rev = speech_rev[0:len(speech_audio)]\n",
        "        wave_data.append(speech_rev)\n",
        "        rev_spec = generate_spec(speech_rev, speech_rate)\n",
        "        \n",
        "        speech_spec = cv2.resize(speech_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "        rev_spec = cv2.resize(rev_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "\n",
        "        print(\"Proccesing audio file n°: \" + str(i+1))\n",
        "        X[i-lower_bound, 0, :, :] = torch.tensor(rev_spec)\n",
        "        y[i-lower_bound, 0, :, :] = torch.tensor(speech_spec)\n",
        "\n",
        "        if ((i+1)%500 == 0):\n",
        "          torch.save(X, checkpointX)\n",
        "          torch.save(y, checkpointY)\n",
        "          torch.save(wave_data, checkpoint_waverev)\n",
        "          torch.save(wave_targets, checkpoint_wavetarget)\n",
        "          print('Saved data')\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdfdjbZqUMDW"
      },
      "outputs": [],
      "source": [
        "def mardy_test_data(audio_dir, rir_dir, lower_bound, upper_bound, checkpoints, snr = 30, distance = 'far'):\n",
        "\n",
        "    \"\"\"\n",
        "    read test data generating reverberant spectrograms and waveforms\n",
        "    \n",
        "    audio_dir: directory containing speech audio files\n",
        "    rir_dir: directory contaning MARDY RIRs\n",
        "    lower_bound: initial example to be considered\n",
        "    upper_bound: final example to be considered\n",
        "    checkpoints: list containing directories for save rev spectrogram, target spectrogram\n",
        "                rev waveforms and target waveforms respectively\n",
        "    snr: add awgn with snr\n",
        "    \"\"\"\n",
        "    \n",
        "    checkpointX = checkpoints[0]\n",
        "    checkpointY = checkpoints[1]\n",
        "    checkpoint_waverev = checkpoints[2]\n",
        "    checkpoint_wavetarget = checkpoints[3]\n",
        "    \n",
        "    sys.path.append(audio_dir)\n",
        "    sys.path.append(rir_dir)\n",
        "\n",
        "    num_distance = '3' if distance == 'far' else '1'\n",
        "    print('Distance Microphones ' + num_distance)\n",
        "    \n",
        "    rir_file_names = []\n",
        "    for subdir, dirs, files in os.walk(rir_dir):\n",
        "        for file in files:\n",
        "            if (\".wav\" in file and file[3]==num_distance):\n",
        "                rir_file_names.append(os.path.join(subdir,file))\n",
        "\n",
        "    audio_file_names = []\n",
        "    for subdir, dirs, files in os.walk(audio_dir):\n",
        "        for file in files:\n",
        "            if (\".flac\" in file):\n",
        "                audio_file_names.append(os.path.join(subdir,file))\n",
        "    \n",
        "    print (\"RIRs found: \" + str(len(rir_file_names)))\n",
        "    print (\"Audio files found: \" + str(len(audio_file_names)))\n",
        "    time_size = 340\n",
        "    frequency_size = 128\n",
        "    X = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    y = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "\n",
        "    wave_data = []\n",
        "    wave_targets = []\n",
        "\n",
        "\n",
        "    for i in range(lower_bound, upper_bound):\n",
        "        rir_index = random.sample(range(len(rir_file_names)), 1)[0]\n",
        "        ir_audio, ir_time, ir_rate = extract_audio(rir_file_names[rir_index])\n",
        "\n",
        "        speech_audio, speech_time, speech_rate = extract_audio(audio_file_names[i])\n",
        "        wave_targets.append(speech_audio)\n",
        "        speech_spec = generate_spec(speech_audio, speech_rate)\n",
        "        speech_rev = discrete_conv(speech_audio, ir_audio, 16000, 48000, aug_factor = 10, snr = snr)\n",
        "        speech_rev = speech_rev[0:len(speech_audio)]\n",
        "        wave_data.append(speech_rev)\n",
        "        rev_spec = generate_spec(speech_rev, speech_rate)\n",
        "        \n",
        "        speech_spec = cv2.resize(speech_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "        rev_spec = cv2.resize(rev_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "\n",
        "        print(\"Proccesing audio file n°: \" + str(i+1))\n",
        "        X[i-lower_bound, 0, :, :] = torch.tensor(rev_spec)\n",
        "        y[i-lower_bound, 0, :, :] = torch.tensor(speech_spec)\n",
        "\n",
        "        if ((i+1)%500 == 0):\n",
        "          torch.save(X, checkpointX)\n",
        "          torch.save(y, checkpointY)\n",
        "          torch.save(wave_data, checkpoint_waverev)\n",
        "          torch.save(wave_targets, checkpoint_wavetarget)\n",
        "          print('Saved data')\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFidRrdnbSoY",
        "outputId": "a926cd4b-fce0-472b-a84b-d5944f57f915"
      },
      "outputs": [],
      "source": [
        "rir_rootdir = 'ClassroomOmni/'\n",
        "audio_rootdir = 'LibriSpeechTest/'\n",
        "checkpointX = '/content/drive/My Drive/data_audio/non_norm_data/X_test.pth'\n",
        "checkpointY = '/content/drive/My Drive/data_audio/non_norm_data/y_test.pth'\n",
        "checkpoint_waverev = '/content/drive/My Drive/data_audio/non_norm_data/waverev.pth'\n",
        "checkpoint_wavetarget = '/content/drive/My Drive/data_audio/non_norm_data/wavetarget.pth'\n",
        "\n",
        "checkpoints = [checkpointX, checkpointY, checkpoint_waverev, checkpoint_wavetarget]\n",
        "\n",
        "X, y = test_data(audio_rootdir, rir_rootdir, 0, 500, checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BEs7SY3CEkA",
        "outputId": "4b3384fb-cbf8-438a-f0ed-82323ce8b3ad"
      },
      "outputs": [],
      "source": [
        "rir_rootdir = '/content/drive/My Drive/data_espec/MARDY'\n",
        "audio_rootdir = 'LibriSpeechTest/'\n",
        "checkpointX = '/content/drive/My Drive/data_audio/non_norm_data/X_test_2.pth'\n",
        "checkpointY = '/content/drive/My Drive/data_audio/non_norm_data/y_test_2.pth'\n",
        "checkpoint_waverev = '/content/drive/My Drive/data_audio/non_norm_data/waverev_2.pth'\n",
        "checkpoint_wavetarget = '/content/drive/My Drive/data_audio/non_norm_data/wavetarget_2.pth'\n",
        "\n",
        "checkpoints = [checkpointX, checkpointY, checkpoint_waverev, checkpoint_wavetarget]\n",
        "\n",
        "X, y = mardy_test_data(audio_rootdir, rir_rootdir, 500, 1000, checkpoints, snr = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EogTX3oeYHxX",
        "outputId": "ffce29e6-3c13-4e1b-8675-88d2e1c94074"
      },
      "outputs": [],
      "source": [
        "rir_rootdir = '/content/drive/My Drive/data_espec/MARDY'\n",
        "audio_rootdir = 'LibriSpeechTest/'\n",
        "checkpointX = '/content/drive/My Drive/data_audio/non_norm_data/X_test_3.pth'\n",
        "checkpointY = '/content/drive/My Drive/data_audio/non_norm_data/y_test_3.pth'\n",
        "checkpoint_waverev = '/content/drive/My Drive/data_audio/non_norm_data/waverev_3.pth'\n",
        "checkpoint_wavetarget = '/content/drive/My Drive/data_audio/non_norm_data/wavetarget_3.pth'\n",
        "\n",
        "checkpoints = [checkpointX, checkpointY, checkpoint_waverev, checkpoint_wavetarget]\n",
        "\n",
        "X, y = mardy_test_data(audio_rootdir, rir_rootdir, 500, 1000, checkpoints, snr = 30, distance = 'near')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7fEijEDogPX",
        "outputId": "d4bfde95-e9ad-49e0-88ba-05744988740b"
      },
      "outputs": [],
      "source": [
        "rir_rootdir = 'ClassroomOmni/'\n",
        "audio_rootdir = 'LibriSpeechTest/'\n",
        "checkpointX = '/content/drive/My Drive/data_audio/non_norm_data/X_test_4.pth'\n",
        "checkpointY = '/content/drive/My Drive/data_audio/non_norm_data/y_test_4.pth'\n",
        "checkpoint_waverev = '/content/drive/My Drive/data_audio/non_norm_data/waverev_4.pth'\n",
        "checkpoint_wavetarget = '/content/drive/My Drive/data_audio/non_norm_data/wavetarget_4.pth'\n",
        "checkpoints = [checkpointX, checkpointY, checkpoint_waverev, checkpoint_wavetarget]\n",
        "X, y = test_data(audio_rootdir, rir_rootdir, 0, 500, checkpoints, noise = [35, 36])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWdMuPpuOEep",
        "outputId": "2e626d7a-048e-4a59-94eb-b2400ef3ef70"
      },
      "outputs": [],
      "source": [
        "rir_rootdir = 'ClassroomOmni/'\n",
        "audio_rootdir = 'LibriSpeechTest/'\n",
        "checkpointX = '/content/drive/My Drive/data_audio/non_norm_data/X_test_5.pth'\n",
        "checkpointY = '/content/drive/My Drive/data_audio/non_norm_data/y_test_5.pth'\n",
        "checkpoint_waverev = '/content/drive/My Drive/data_audio/non_norm_data/waverev_5.pth'\n",
        "checkpoint_wavetarget = '/content/drive/My Drive/data_audio/non_norm_data/wavetarget_5.pth'\n",
        "checkpoints = [checkpointX, checkpointY, checkpoint_waverev, checkpoint_wavetarget]\n",
        "X, y = test_data(audio_rootdir, rir_rootdir, 0, 500, checkpoints, noise = [15, 16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uksUsREDk5NX"
      },
      "outputs": [],
      "source": [
        "def realdata_from_dir(audio_dir, lower_bound, upper_bound, checkpointX, checkpoint_wave):\n",
        "    \"\"\"\n",
        "    read real test data with reverberant spectrograms and waveforms\n",
        "    \n",
        "    audio_dir: directory containing speech audio files\n",
        "    lower_bound: initial example to be considered\n",
        "    upper_bound: final example to be considered\n",
        "    checkpointX: directory + filename to save reverberant spectrograms\n",
        "    checkpointX: directory + filename to save reverberant waveforms\n",
        "    \"\"\"\n",
        "    \n",
        "    sys.path.append(audio_dir)\n",
        "\n",
        "    audio_file_names = []\n",
        "    for subdir, dirs, files in os.walk(audio_dir):\n",
        "        for file in files:\n",
        "            if (\".wav\" in file):\n",
        "                audio_file_names.append(os.path.join(subdir,file))\n",
        "\n",
        "    print (\"Archivos de audio encontrados: \" + str(len(audio_file_names)))\n",
        "    time_size = 340\n",
        "    frequency_size = 128\n",
        "\n",
        "    X = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    y = torch.zeros((upper_bound-lower_bound, 1, frequency_size, time_size))\n",
        "    \n",
        "    waves_rev = []\n",
        "    for i in range(lower_bound, upper_bound):\n",
        "\n",
        "        speech_rev, speech_time, speech_rate = extract_audio(audio_file_names[i])\n",
        "        waves_rev.append(speech_rev)\n",
        "        rev_spec = generate_spec(speech_rev, speech_rate)\n",
        "        rev_spec = cv2.resize(rev_spec, dsize = (time_size, frequency_size), interpolation = cv2.INTER_LANCZOS4)\n",
        "\n",
        "        print(\"Procesado archivo de audio n°: \" + str(i+1))\n",
        "        X[i-lower_bound, 0, :, :] = torch.tensor(rev_spec)\n",
        "  \n",
        "        if ((i+1)%50 == 0):\n",
        "          torch.save(X, checkpointX)\n",
        "          torch.save(waves_rev, checkpoint_wave)\n",
        "          print('Saved data')\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO9lkBnCm7k1",
        "outputId": "7ba401fc-abcb-4379-c585-305e536e1fb7"
      },
      "outputs": [],
      "source": [
        "audio_rootdir = '/content/drive/My Drive/masive_data/VUT_FIT_L207/MicID01/SpkID01_20171225_T/01/'\n",
        "checkpointX = '/content/drive/My Drive/real_data/X_test_real1.pth'\n",
        "checkpoint_wave = '/content/drive/My Drive/real_data/waves1.pth'\n",
        "X = realdata_from_dir(audio_rootdir, 0, 500, checkpointX, checkpoint_wave)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1a4BDGJ6YHs",
        "outputId": "a8996606-9be1-46d1-c44e-c93a1f001cbc"
      },
      "outputs": [],
      "source": [
        "audio_rootdir = '/content/drive/My Drive/masive_data/VUT_FIT_L207/MicID01/SpkID01_20171225_T/10/'\n",
        "checkpointX = '/content/drive/My Drive/real_data/X_test_real2.pth'\n",
        "checkpoint_wave = '/content/drive/My Drive/real_data/waves2.pth'\n",
        "X = realdata_from_dir(audio_rootdir, 0, 500, checkpointX, checkpoint_wave)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generate_spectrograms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "taub_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9f6e8cd43113ef1772f0e84106355c3897590e0412fa325c21bda7b78104f9ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
